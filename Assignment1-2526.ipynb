{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qE4WC2_4wygJ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignment 1\n",
    "**Credits**: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\n",
    "\n",
    "**Keywords**: Sexism Detection, Multi-class Classification, RNNs, Transformers, Huggingface\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fL69zGpmx01k",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Contact\n",
    "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
    "\n",
    "Teaching Assistants:\n",
    "\n",
    "- Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
    "- Eleonora Mancini -> e.mancini@unibo.it\n",
    "\n",
    "Professor:\n",
    "- Paolo Torroni -> p.torroni@unibo.it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55jnW-xKxi-2",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "You are asked to address the [EXIST 2023 Task 2](https://clef2023.clef-initiative.eu/index.php?page=Pages/labs.html#EXIST) on sexism detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HWp5bGwySsb",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Problem Definition\n",
    "\n",
    "This task aims to categorize the sexist messages according to the intention of the author in one of the following categories: (i) direct sexist message, (ii) reported sexist message and (iii) judgemental message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Examples:\n",
    "\n",
    "#### DIRECT \n",
    "The intention was to write a message that is sexist by itself or incites to be sexist, as in:\n",
    "\n",
    "''*A woman needs love, to fill the fridge, if a man can give this to her in return for her services (housework, cooking, etc), I don’t see what else she needs.*''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### REPORTED\n",
    "The intention is to report and share a sexist situation suffered by a woman or women in first or third person, as in:\n",
    "\n",
    "''*Today, one of my year 1 class pupils could not believe he’d lost a race against a girl.*''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### JUDGEMENTAL\n",
    "The intention was to judge, since the tweet describes sexist situations or behaviours with the aim of condemning them.\n",
    "\n",
    "''*As usual, the woman was the one quitting her job for the family’s welfare…*''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iu1X4I98M8B",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 1 - 1.0 points] Corpus\n",
    "\n",
    "We have preparared a small version of EXIST dataset in our dedicated [Github repository](https://github.com/lt-nlp-lab-unibo/nlp-course-material/tree/main/2025-2026/Assignment%201/data).\n",
    "\n",
    "Check the `A1/data` folder. It contains 3 `.json` files representing `training`, `validation` and `test` sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AASoMV9XN5l6",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dataset Description\n",
    "- The dataset contains tweets in both English and Spanish.\n",
    "- There are label for multiple tasks, but we are focusing on **Task 2**.\n",
    "- For Task 2, label are assigned by six annotators.\n",
    "- The label for Task 2 represent whether the tweet is non-sexist ('-') or its sexist intention ('DIRECT', 'REPORTED', 'JUDGEMENTAL').\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFjwB_lCOQKj",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Example\n",
    "\n",
    "```\n",
    "    \"203260\": {\n",
    "        \"id_EXIST\": \"203260\",\n",
    "        \"lang\": \"en\",\n",
    "        \"tweet\": \"ik when mandy says “you look like a whore” i look cute as FUCK\",\n",
    "        \"number_annotators\": 6,\n",
    "        \"annotators\": [\"Annotator_473\", \"Annotator_474\", \"Annotator_475\", \"Annotator_476\", \"Annotator_477\", \"Annotator_27\"],\n",
    "        \"gender_annotators\": [\"F\", \"F\", \"M\", \"M\", \"M\", \"F\"],\n",
    "        \"age_annotators\": [\"18-22\", \"23-45\", \"18-22\", \"23-45\", \"46+\", \"46+\"],\n",
    "        \"label_task1\": [\"YES\", \"YES\", \"YES\", \"NO\", \"YES\", \"YES\"],\n",
    "        \"labels_task2\": [\"DIRECT\", \"DIRECT\", \"REPORTED\", \"-\", \"JUDGEMENTAL\", \"REPORTED\"],\n",
    "        \"labels_task3\": [\n",
    "          [\"STEREOTYPING-DOMINANCE\"],\n",
    "          [\"OBJECTIFICATION\"],\n",
    "          [\"SEXUAL-VIOLENCE\"],\n",
    "          [\"-\"],\n",
    "          [\"STEREOTYPING-DOMINANCE\", \"OBJECTIFICATION\"],\n",
    "          [\"OBJECTIFICATION\"]\n",
    "        ],\n",
    "        \"split\": \"TRAIN_EN\"\n",
    "      }\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJ45bvuOOJ7I",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Instructions\n",
    "1. **Download** the `A1/data` folder.\n",
    "2. **Load** the three JSON files and encode them as ``pandas.DataFrame``.\n",
    "3. **Aggregate labels** for Task 2 using majority voting and store them in a new dataframe column called `label`. Items without a clear majority will be removed from the dataset.\n",
    "4. **Filter the DataFrame** to keep only rows where the `lang` column is `'en'`.\n",
    "5. **Remove unwanted columns**: Keep only `id_EXIST`, `lang`, `tweet`, and `label`.\n",
    "6. **Encode the `label` column**: Use the following mapping\n",
    "\n",
    "```\n",
    "{\n",
    "    '-': 0,\n",
    "    'DIRECT': 1,\n",
    "    'JUDGEMENTAL': 2,\n",
    "    'REPORTED': 3\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6920 records from data/training.json\n",
      "Loaded 312 records from data/test.json\n",
      "Loaded 726 records from data/validation.json\n"
     ]
    }
   ],
   "source": [
    "path_train = \"data/training.json\"\n",
    "df_train = pd.read_json(path_train)\n",
    "df_train = df_train.reset_index(drop=False).rename(columns={\"index\": \"key\"}).transpose()\n",
    "df_train.columns = df_train.iloc[0]\n",
    "df_train = df_train[1:]\n",
    "df_train.head()\n",
    "print(f\"Loaded {len(df_train)} records from {path_train}\")\n",
    "# df_train.head()\n",
    "\n",
    "path_test = \"data/test.json\"\n",
    "df_test = pd.read_json(path_test)\n",
    "df_test = df_test.reset_index(drop=False).rename(columns={\"index\": \"key\"}).transpose()\n",
    "df_test.columns = df_test.iloc[0]\n",
    "df_test = df_test[1:]\n",
    "df_test.head()\n",
    "print(f\"Loaded {len(df_test)} records from {path_test}\")\n",
    "# df_test.head()\n",
    "\n",
    "path_val = \"data/validation.json\"\n",
    "df_val = pd.read_json(path_val)\n",
    "df_val = df_val.reset_index(drop=False).rename(columns={\"index\": \"key\"}).transpose()\n",
    "df_val.columns = df_val.iloc[0]\n",
    "df_val = df_val[1:]\n",
    "df_val.head()\n",
    "print(f\"Loaded {len(df_val)} records from {path_val}\")\n",
    "# df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>key</th>\n",
       "      <th>id_EXIST</th>\n",
       "      <th>lang</th>\n",
       "      <th>tweet</th>\n",
       "      <th>number_annotators</th>\n",
       "      <th>annotators</th>\n",
       "      <th>gender_annotators</th>\n",
       "      <th>age_annotators</th>\n",
       "      <th>labels_task1</th>\n",
       "      <th>labels_task2</th>\n",
       "      <th>labels_task3</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>300001</th>\n",
       "      <td>300001</td>\n",
       "      <td>es</td>\n",
       "      <td>@Fichinescu La comunidad gamer es un antro de ...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_726, Annotator_727, Annotator_357, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
       "      <td>[NO, YES, YES, NO, YES, NO]</td>\n",
       "      <td>[-, JUDGEMENTAL, JUDGEMENTAL, -, REPORTED, -]</td>\n",
       "      <td>[[-], [MISOGYNY-NON-SEXUAL-VIOLENCE], [MISOGYN...</td>\n",
       "      <td>DEV_ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300002</th>\n",
       "      <td>300002</td>\n",
       "      <td>es</td>\n",
       "      <td>@anacaotica88 @MordorLivin No me acuerdo de lo...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_731, Annotator_732, Annotator_315, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
       "      <td>[YES, YES, NO, YES, YES, YES]</td>\n",
       "      <td>[JUDGEMENTAL, REPORTED, -, JUDGEMENTAL, JUDGEM...</td>\n",
       "      <td>[[IDEOLOGICAL-INEQUALITY, STEREOTYPING-DOMINAN...</td>\n",
       "      <td>DEV_ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300003</th>\n",
       "      <td>300003</td>\n",
       "      <td>es</td>\n",
       "      <td>@cosmicJunkBot lo digo cada pocos dias y lo re...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_735, Annotator_736, Annotator_345, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
       "      <td>[NO, NO, NO, NO, NO, NO]</td>\n",
       "      <td>[-, -, -, -, -, -]</td>\n",
       "      <td>[[-], [-], [-], [-], [-], [-]]</td>\n",
       "      <td>DEV_ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300004</th>\n",
       "      <td>300004</td>\n",
       "      <td>es</td>\n",
       "      <td>Also mientras les decia eso la señalaba y deci...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_259, Annotator_739, Annotator_291, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
       "      <td>[NO, YES, YES, YES, YES, YES]</td>\n",
       "      <td>[-, REPORTED, REPORTED, REPORTED, JUDGEMENTAL,...</td>\n",
       "      <td>[[-], [SEXUAL-VIOLENCE], [SEXUAL-VIOLENCE], [S...</td>\n",
       "      <td>DEV_ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300005</th>\n",
       "      <td>300005</td>\n",
       "      <td>es</td>\n",
       "      <td>And all people killed,  attacked, harassed by ...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_731, Annotator_732, Annotator_315, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
       "      <td>[NO, YES, NO, NO, NO, NO]</td>\n",
       "      <td>[-, DIRECT, -, -, -, -]</td>\n",
       "      <td>[[-], [STEREOTYPING-DOMINANCE], [-], [-], [-],...</td>\n",
       "      <td>DEV_ES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "key    id_EXIST lang                                              tweet  \\\n",
       "300001   300001   es  @Fichinescu La comunidad gamer es un antro de ...   \n",
       "300002   300002   es  @anacaotica88 @MordorLivin No me acuerdo de lo...   \n",
       "300003   300003   es  @cosmicJunkBot lo digo cada pocos dias y lo re...   \n",
       "300004   300004   es  Also mientras les decia eso la señalaba y deci...   \n",
       "300005   300005   es  And all people killed,  attacked, harassed by ...   \n",
       "\n",
       "key    number_annotators                                         annotators  \\\n",
       "300001                 6  [Annotator_726, Annotator_727, Annotator_357, ...   \n",
       "300002                 6  [Annotator_731, Annotator_732, Annotator_315, ...   \n",
       "300003                 6  [Annotator_735, Annotator_736, Annotator_345, ...   \n",
       "300004                 6  [Annotator_259, Annotator_739, Annotator_291, ...   \n",
       "300005                 6  [Annotator_731, Annotator_732, Annotator_315, ...   \n",
       "\n",
       "key      gender_annotators                          age_annotators  \\\n",
       "300001  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
       "300002  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
       "300003  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
       "300004  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
       "300005  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
       "\n",
       "key                      labels_task1  \\\n",
       "300001    [NO, YES, YES, NO, YES, NO]   \n",
       "300002  [YES, YES, NO, YES, YES, YES]   \n",
       "300003       [NO, NO, NO, NO, NO, NO]   \n",
       "300004  [NO, YES, YES, YES, YES, YES]   \n",
       "300005      [NO, YES, NO, NO, NO, NO]   \n",
       "\n",
       "key                                          labels_task2  \\\n",
       "300001      [-, JUDGEMENTAL, JUDGEMENTAL, -, REPORTED, -]   \n",
       "300002  [JUDGEMENTAL, REPORTED, -, JUDGEMENTAL, JUDGEM...   \n",
       "300003                                 [-, -, -, -, -, -]   \n",
       "300004  [-, REPORTED, REPORTED, REPORTED, JUDGEMENTAL,...   \n",
       "300005                            [-, DIRECT, -, -, -, -]   \n",
       "\n",
       "key                                          labels_task3   split  \n",
       "300001  [[-], [MISOGYNY-NON-SEXUAL-VIOLENCE], [MISOGYN...  DEV_ES  \n",
       "300002  [[IDEOLOGICAL-INEQUALITY, STEREOTYPING-DOMINAN...  DEV_ES  \n",
       "300003                     [[-], [-], [-], [-], [-], [-]]  DEV_ES  \n",
       "300004  [[-], [SEXUAL-VIOLENCE], [SEXUAL-VIOLENCE], [S...  DEV_ES  \n",
       "300005  [[-], [STEREOTYPING-DOMINANCE], [-], [-], [-],...  DEV_ES  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>key</th>\n",
       "      <th>id_EXIST</th>\n",
       "      <th>lang</th>\n",
       "      <th>tweet</th>\n",
       "      <th>number_annotators</th>\n",
       "      <th>annotators</th>\n",
       "      <th>gender_annotators</th>\n",
       "      <th>age_annotators</th>\n",
       "      <th>labels_task1</th>\n",
       "      <th>labels_task2</th>\n",
       "      <th>labels_task3</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>400178</th>\n",
       "      <td>400178</td>\n",
       "      <td>en</td>\n",
       "      <td>1st day at the pool on a beautiful Sunday in N...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_764, Annotator_765, Annotator_766, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
       "      <td>[NO, NO, NO, NO, NO, NO]</td>\n",
       "      <td>[-, -, -, -, -, -]</td>\n",
       "      <td>[[-], [-], [-], [-], [-], [-]]</td>\n",
       "      <td>DEV_EN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400179</th>\n",
       "      <td>400179</td>\n",
       "      <td>en</td>\n",
       "      <td>“I like your outfit too except when i dress up...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_805, Annotator_426, Annotator_806, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
       "      <td>[YES, YES, YES, YES, YES, NO]</td>\n",
       "      <td>[JUDGEMENTAL, DIRECT, REPORTED, DIRECT, REPORT...</td>\n",
       "      <td>[[OBJECTIFICATION], [OBJECTIFICATION, MISOGYNY...</td>\n",
       "      <td>DEV_EN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400180</th>\n",
       "      <td>400180</td>\n",
       "      <td>en</td>\n",
       "      <td>@KNasFanFic 🥺💖 same, though!!! the angst just ...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_795, Annotator_796, Annotator_797, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
       "      <td>[NO, NO, NO, NO, NO, NO]</td>\n",
       "      <td>[-, -, -, -, -, -]</td>\n",
       "      <td>[[-], [-], [-], [-], [-], [-]]</td>\n",
       "      <td>DEV_EN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400181</th>\n",
       "      <td>400181</td>\n",
       "      <td>en</td>\n",
       "      <td>@themaxburns @GOP Fuck that cunt. Tried to vot...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_795, Annotator_796, Annotator_797, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
       "      <td>[NO, YES, YES, YES, YES, YES]</td>\n",
       "      <td>[-, DIRECT, JUDGEMENTAL, DIRECT, DIRECT, DIRECT]</td>\n",
       "      <td>[[-], [IDEOLOGICAL-INEQUALITY, MISOGYNY-NON-SE...</td>\n",
       "      <td>DEV_EN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400182</th>\n",
       "      <td>400182</td>\n",
       "      <td>en</td>\n",
       "      <td>@ultshunnie u gotta say some shit like “i’ll f...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_770, Annotator_771, Annotator_772, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
       "      <td>[YES, YES, YES, YES, YES, YES]</td>\n",
       "      <td>[DIRECT, REPORTED, DIRECT, DIRECT, JUDGEMENTAL...</td>\n",
       "      <td>[[OBJECTIFICATION, SEXUAL-VIOLENCE], [SEXUAL-V...</td>\n",
       "      <td>DEV_EN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "key    id_EXIST lang                                              tweet  \\\n",
       "400178   400178   en  1st day at the pool on a beautiful Sunday in N...   \n",
       "400179   400179   en  “I like your outfit too except when i dress up...   \n",
       "400180   400180   en  @KNasFanFic 🥺💖 same, though!!! the angst just ...   \n",
       "400181   400181   en  @themaxburns @GOP Fuck that cunt. Tried to vot...   \n",
       "400182   400182   en  @ultshunnie u gotta say some shit like “i’ll f...   \n",
       "\n",
       "key    number_annotators                                         annotators  \\\n",
       "400178                 6  [Annotator_764, Annotator_765, Annotator_766, ...   \n",
       "400179                 6  [Annotator_805, Annotator_426, Annotator_806, ...   \n",
       "400180                 6  [Annotator_795, Annotator_796, Annotator_797, ...   \n",
       "400181                 6  [Annotator_795, Annotator_796, Annotator_797, ...   \n",
       "400182                 6  [Annotator_770, Annotator_771, Annotator_772, ...   \n",
       "\n",
       "key      gender_annotators                          age_annotators  \\\n",
       "400178  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
       "400179  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
       "400180  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
       "400181  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
       "400182  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
       "\n",
       "key                       labels_task1  \\\n",
       "400178        [NO, NO, NO, NO, NO, NO]   \n",
       "400179   [YES, YES, YES, YES, YES, NO]   \n",
       "400180        [NO, NO, NO, NO, NO, NO]   \n",
       "400181   [NO, YES, YES, YES, YES, YES]   \n",
       "400182  [YES, YES, YES, YES, YES, YES]   \n",
       "\n",
       "key                                          labels_task2  \\\n",
       "400178                                 [-, -, -, -, -, -]   \n",
       "400179  [JUDGEMENTAL, DIRECT, REPORTED, DIRECT, REPORT...   \n",
       "400180                                 [-, -, -, -, -, -]   \n",
       "400181   [-, DIRECT, JUDGEMENTAL, DIRECT, DIRECT, DIRECT]   \n",
       "400182  [DIRECT, REPORTED, DIRECT, DIRECT, JUDGEMENTAL...   \n",
       "\n",
       "key                                          labels_task3   split  \n",
       "400178                     [[-], [-], [-], [-], [-], [-]]  DEV_EN  \n",
       "400179  [[OBJECTIFICATION], [OBJECTIFICATION, MISOGYNY...  DEV_EN  \n",
       "400180                     [[-], [-], [-], [-], [-], [-]]  DEV_EN  \n",
       "400181  [[-], [IDEOLOGICAL-INEQUALITY, MISOGYNY-NON-SE...  DEV_EN  \n",
       "400182  [[OBJECTIFICATION, SEXUAL-VIOLENCE], [SEXUAL-V...  DEV_EN  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the new column 'label'\n",
    "def most_common_label(labels):\n",
    "\n",
    "    if ((len(Counter(labels))>1) and (Counter(labels).most_common(1)[0][1] == Counter(labels).most_common(2)[1][1])):\n",
    "        return pd.NA\n",
    "    return Counter(labels).most_common(1)[0][0]\n",
    "    \n",
    "# Populating the new column with the most common label for each row\n",
    "df_train['label'] = df_train['labels_task2'].apply(most_common_label)\n",
    "df_test['label'] = df_test['labels_task2'].apply(most_common_label)\n",
    "df_val['label'] = df_val['labels_task2'].apply(most_common_label)\n",
    "\n",
    "# Deleting all the rows with no most common label\n",
    "df_train_dropna = df_train.dropna(subset=['label'])    \n",
    "df_test_dropna = df_test.dropna(subset=['label'])    \n",
    "df_val_dropna = df_val.dropna(subset=['label'])\n",
    "\n",
    "# Filtering only English tweets and selecting relevant columns\n",
    "df_train_en = df_train_dropna[df_train_dropna['lang'] == 'en'][['id_EXIST','lang','tweet','label']]\n",
    "df_test_en = df_test_dropna[df_test_dropna['lang']=='en'][['id_EXIST','lang','tweet','label']]\n",
    "df_val_en = df_val_dropna[df_val_dropna['lang']=='en'][['id_EXIST','lang','tweet','label']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping labels to integers\n",
    "mapping = {\n",
    "    '-': 0,\n",
    "    'DIRECT': 1,\n",
    "    'JUDGEMENTAL': 2,\n",
    "    'REPORTED': 3\n",
    "}\n",
    "\n",
    "df_train_en['label'] = df_train_en['label'].map(mapping)\n",
    "df_test_en['label'] = df_test_en['label'].map(mapping)\n",
    "df_val_en['label'] = df_val_en['label'].map(mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>key</th>\n",
       "      <th>id_EXIST</th>\n",
       "      <th>lang</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200001</th>\n",
       "      <td>200001</td>\n",
       "      <td>en</td>\n",
       "      <td>FFS! How about laying the blame on the bastard...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200002</th>\n",
       "      <td>200002</td>\n",
       "      <td>en</td>\n",
       "      <td>Writing a uni essay in my local pub with a cof...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200003</th>\n",
       "      <td>200003</td>\n",
       "      <td>en</td>\n",
       "      <td>@UniversalORL it is 2021 not 1921. I dont appr...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200004</th>\n",
       "      <td>200004</td>\n",
       "      <td>en</td>\n",
       "      <td>@GMB this is unacceptable. Use her title as yo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200005</th>\n",
       "      <td>200005</td>\n",
       "      <td>en</td>\n",
       "      <td>‘Making yourself a harder target’ basically bo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "key    id_EXIST lang                                              tweet  label\n",
       "200001   200001   en  FFS! How about laying the blame on the bastard...      0\n",
       "200002   200002   en  Writing a uni essay in my local pub with a cof...      3\n",
       "200003   200003   en  @UniversalORL it is 2021 not 1921. I dont appr...      3\n",
       "200004   200004   en  @GMB this is unacceptable. Use her title as yo...      0\n",
       "200005   200005   en  ‘Making yourself a harder target’ basically bo...      0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_en.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task2 - 0.5 points] Data Cleaning\n",
    "In the context of tweets, we have noisy and informal data that often includes unnecessary elements like emojis, hashtags, mentions, and URLs. These elements may interfere with the text analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Instructions\n",
    "- **Remove emojis** from the tweets.\n",
    "- **Remove hashtags** (e.g., `#example`).\n",
    "- **Remove mentions** such as `@user`.\n",
    "- **Remove URLs** from the tweets.\n",
    "- **Remove special characters and symbols**.\n",
    "- **Remove specific quote characters** (e.g., curly quotes).\n",
    "- **Perform lemmatization** to reduce words to their base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\n",
    "    \"[\"\n",
    "    \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    \"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "    \"\\U00002700-\\U000027BF\"\n",
    "    \"\\U000024C2-\\U0001F251\"\n",
    "    \"]+\",\n",
    "    flags=re.UNICODE\n",
    ")\n",
    "\n",
    "def remove_emojis_regex(text):\n",
    "    return emoji_pattern.sub(r'', str(text))\n",
    "\n",
    "def remove_special_chars(text):\n",
    "    # normalize unicode (compatibility composition)\n",
    "    text = unicodedata.normalize('NFKC', str(text))\n",
    "    # unify/remove smart quotes\n",
    "    text = text.replace('“', '\"').replace('”', '\"').replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "    # keep letters (including Latin-1 letters), digits, basic punctuation and spaces\n",
    "    text = re.sub(r\"[^0-9A-Za-zÀ-ÖØ-öø-ÿ\\s\\.\\,\\!\\?\\:\\;\\'\\\"\\-\\(\\)]\", \"\", text)\n",
    "    # collapse multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', str(text))  # remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)         # remove mentions\n",
    "    text = re.sub(r'#\\w+', '', text)         # remove hashtags\n",
    "    text = remove_special_chars(text)         # remove other special characters/symbols\n",
    "    text = remove_emojis_regex(text)          # remove emojis\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_en['tweet'] = df_train_en['tweet'].apply(clean_text)\n",
    "df_val_en['tweet']   = df_val_en['tweet'].apply(clean_text)\n",
    "df_test_en['tweet']  = df_test_en['tweet'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200001    FFS! How about laying the blame on the bastard...\n",
       "200002    Writing a uni essay in my local pub with a cof...\n",
       "200003    it is 2021 not 1921. I dont appreciate that on...\n",
       "200004    this is unacceptable. Use her title as you did...\n",
       "200005    'Making yourself a harder target' basically bo...\n",
       "                                ...                        \n",
       "203254    Ma'am if I say that you look like a whore,Woul...\n",
       "203256    idk why y'all bitches think having half your a...\n",
       "203257    This has been a part of an experiment with . W...\n",
       "203258    \"Take me already\" \"Not yet. You gotta be ready...\n",
       "203259                     why do you look like a whore? lh\n",
       "Name: tweet, Length: 2873, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_en[\"tweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\dales\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\dales\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dales\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "wn_map = {'J': wordnet.ADJ, 'V': wordnet.VERB, 'N': wordnet.NOUN, 'R': wordnet.ADV}\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def pos_to_wordnet(tag):\n",
    "    return wn_map.get(tag[0], wordnet.NOUN)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    tokens = word_tokenize(str(text))\n",
    "    tagged = pos_tag(tokens)\n",
    "    lemmas = [lemmatizer.lemmatize(tok, pos=pos_to_wordnet(tg)) for tok, tg in tagged]\n",
    "    return \" \".join([l for l in lemmas if l.strip()])\n",
    "\n",
    "df_train_en['tweet'] = df_train_en['tweet'].apply(lemmatize_text)\n",
    "df_val_en['tweet']   = df_val_en['tweet'].apply(lemmatize_text)\n",
    "df_test_en['tweet']  = df_test_en['tweet'].apply(lemmatize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200001    FFS ! How about lay the blame on the bastard w...\n",
       "200002    Writing a uni essay in my local pub with a cof...\n",
       "200003    it be 2021 not 1921 . I dont appreciate that o...\n",
       "200004    this be unacceptable . Use her title a you do ...\n",
       "200005    'Making yourself a hard target ' basically boi...\n",
       "                                ...                        \n",
       "203254    Ma'am if I say that you look like a whore , Wo...\n",
       "203256    idk why y'all bitch think have half your as ha...\n",
       "203257    This have be a part of an experiment with . Wh...\n",
       "203258    `` Take me already '' `` Not yet . You get ta ...\n",
       "203259                    why do you look like a whore ? lh\n",
       "Name: tweet, Length: 2873, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_en[\"tweet\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3KylLHNl0bE",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 3 - 0.5 points] Text Encoding\n",
    "To train a neural sexism classifier, you first need to encode text into numerical format.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hr1lTHUVOXff",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Embed words using **GloVe embeddings**.\n",
    "* You are **free** to pick any embedding dimension.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6NNMEjWOZQr",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What about OOV tokens?\n",
    "   * All the tokens in the **training** set that are not in GloVe **must** be added to the vocabulary.\n",
    "   * For the remaining tokens (i.e., OOV in the validation and test sets), you have to assign them a **special token** (e.g., ``<UNK>``) and a **static** embedding.\n",
    "   * You are **free** to define the static embedding using any strategy (e.g., random, neighbourhood, etc...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90UztlGUObXk",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### More about OOV\n",
    "\n",
    "For a given token:\n",
    "\n",
    "* **If in train set**: add to vocabulary and assign an embedding (use GloVe if token in GloVe, custom embedding otherwise).\n",
    "* **If in val/test set**: assign special token if not in vocabulary and assign custom embedding.\n",
    "\n",
    "Your vocabulary **should**:\n",
    "\n",
    "* Contain all tokens in train set; or\n",
    "* Union of tokens in train set and in GloVe $\\rightarrow$ we make use of existing knowledge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JLnuLGHGAUT",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 4 - 1.0 points] Model definition\n",
    "\n",
    "You are now tasked to define your sexism classifier.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQFI9J-JOfXD",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* **Baseline**: implement a Bidirectional LSTM with a Dense layer on top.\n",
    "\n",
    "* **Stacked**: add an additional Bidirectional LSTM layer to the Baseline model.\n",
    "\n",
    "**Note**: You are **free** to experiment with hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jALc_qYGS2E",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Token to embedding mapping\n",
    "\n",
    "You can follow two approaches for encoding tokens in your classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Work directly with embeddings\n",
    "\n",
    "- Compute the embedding of each input token\n",
    "- Feed the mini-batches of shape ``(batch_size, # tokens, embedding_dim)`` to your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Work with Embedding layer\n",
    "\n",
    "- Encode input tokens to token ids\n",
    "- Define a Embedding layer as the first layer of your model\n",
    "- Compute the embedding matrix of all known tokens (i.e., tokens in your vocabulary)\n",
    "- Initialize the Embedding layer with the computed embedding matrix\n",
    "- You are **free** to set the Embedding layer trainable or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Z44PckrNGfTv",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m embedding = \u001b[43mtf\u001b[49m.keras.layers.Embedding(input_dim=vocab_size,\n\u001b[32m      2\u001b[39m                                       output_dim=embedding_dimension,\n\u001b[32m      3\u001b[39m                                       weights=[embedding_matrix],\n\u001b[32m      4\u001b[39m                                       mask_zero=\u001b[38;5;28;01mTrue\u001b[39;00m,                   \u001b[38;5;66;03m# automatically masks padding tokens\u001b[39;00m\n\u001b[32m      5\u001b[39m                                       name=\u001b[33m'\u001b[39m\u001b[33mencoder_embedding\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "embedding = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "                                      output_dim=embedding_dimension,\n",
    "                                      weights=[embedding_matrix],\n",
    "                                      mask_zero=True,                   # automatically masks padding tokens\n",
    "                                      name='encoder_embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFjBgdiRG3wD",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 5 - 1.0 points] Training and Evaluation\n",
    "\n",
    "You are now tasked to train and evaluate the Baseline and Stacked models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWPK4umGOjtT",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Instructions\n",
    "\n",
    "* Pick **at least** three seeds for robust estimation.\n",
    "* Train **all** models on the train set.\n",
    "* Evaluate **all** models on the validation set.\n",
    "* Compute macro F1-score, precision, and recall metrics on the validation set.\n",
    "* Report average and standard deviation measures over seeds for each metric.\n",
    "* Pick the **best** performing model according to the observed validation set performance (use macro F1-score)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSy9sPwYHUoD",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 6 - 1.0 points] Transformers\n",
    "\n",
    "In this section, you will use a transformer model specifically trained for hate speech detection, namely [Twitter-roBERTa-base for Hate Speech Detection](https://huggingface.co/cardiffnlp/twitter-roberta-base-hate).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Relevant Material\n",
    "- Tutorial 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Instructions\n",
    "- **Load the Tokenizer and Model**\n",
    "\n",
    "- **Preprocess the Dataset**:\n",
    "   You will need to preprocess your dataset to prepare it for input into the model. Tokenize your text data using the appropriate tokenizer and ensure it is formatted correctly.\n",
    "\n",
    "- **Train the Model**:\n",
    "   Use the `Trainer` to train the model on your training data.\n",
    "\n",
    "- **Evaluate the Model on the Test Set** using the same metrics used for LSTM-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gtiG2mAL3HM",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 7 - 0.5 points] Error Analysis\n",
    "\n",
    "After evaluating the model, perform a brief error analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    " - Review the results and identify common errors.\n",
    "\n",
    " - Summarize your findings regarding the errors and their impact on performance (e.g. but not limited to Out-of-Vocabulary (OOV) words, data imbalance, and performance differences between the custom model and the transformer...)\n",
    " - Suggest possible solutions to address the identified errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P42XYjb6K3k5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 8 - 0.5 points] Report\n",
    "\n",
    "Wrap up your experiment in a short report (up to 2 pages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9oXSaW1K5S7",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Use the NLP course report template.\n",
    "* Summarize each task in the report following the provided template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHw2L6PlLDyE",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Recommendations\n",
    "\n",
    "The report is **not a copy-paste** of graphs, tables, and command outputs.\n",
    "\n",
    "* Summarize classification performance in Table format.\n",
    "* **Do not** report command outputs or screenshots.\n",
    "* Report learning curves in Figure format.\n",
    "* The error analysis section should summarize your findings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMUqh1utLflM",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Submission\n",
    "\n",
    "* **Submit** your report in PDF format.\n",
    "* **Submit** your python notebook.\n",
    "* Make sure your notebook is **well organized**, with no temporary code, commented sections, tests, etc...\n",
    "* You can upload **model weights** in a cloud repository and report the link in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bonus Points\n",
    "Bonus points are arbitrarily assigned based on significant contributions such as:\n",
    "- Outstanding error analysis\n",
    "- Masterclass code organization\n",
    "- Suitable extensions\n",
    "\n",
    "**Note**: bonus points are only assigned if all task points are attributed (i.e., 6/6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Possible Suggestions for Bonus Points:**\n",
    "- **Try other preprocessing strategies**: e.g., but not limited to, explore techniques tailored specifically for tweets or  methods that are common in social media text.\n",
    "- **Experiment with other custom architectures or models from HuggingFace**\n",
    "- **Explore Spanish tweets**: e.g., but not limited to, leverage multilingual models to process Spanish tweets and assess their performance compared to monolingual models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypagJed7LheY",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# FAQ\n",
    "\n",
    "Please check this frequently asked questions before contacting us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BjMk5e_M4n7",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Trainable Embeddings\n",
    "\n",
    "You are **free** to define a trainable or non-trainable Embedding layer to load the GloVe embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8TVgpYlM6s5",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model architecture\n",
    "\n",
    "You **should not** change the architecture of a model (i.e., its layers).\n",
    "\n",
    "However, you are **free** to play with their hyper-parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ia6IapI1M_A7",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Neural Libraries\n",
    "\n",
    "You are **free** to use any library of your choice to implement the networks (e.g., Keras, Tensorflow, PyTorch, JAX, etc...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1WcrpemNEQm",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Robust Evaluation\n",
    "\n",
    "Each model is trained with at least 3 random seeds.\n",
    "\n",
    "Task 5 requires you to compute the average performance over the 3 seeds and its corresponding standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Expected Results\n",
    "\n",
    "Task 2 leaderboard reports around 40-50 F1-score.\n",
    "However, note that they perform a hierarchical classification.\n",
    "\n",
    "That said, results around 30-40 F1-score are **expected** given the task's complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mVe5dqzNI_u",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model Selection for Analysis\n",
    "\n",
    "To carry out the error analysis you are **free** to either\n",
    "\n",
    "* Pick examples or perform comparisons with an individual seed run model (e.g., Baseline seed 1337)\n",
    "* Perform ensembling via, for instance, majority voting to obtain a single model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8a4pDKSNKzI",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Error Analysis\n",
    "\n",
    "Some topics for discussion include:\n",
    "   * Precision/Recall curves.\n",
    "   * Confusion matrices.\n",
    "   * Specific misclassified samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xmMKE7vLu-y",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# The End\n",
    "\n",
    "Feel free to reach out for questions/doubts!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "authorship_tag": "ABX9TyOrxI+uaofLerXT9gw1DmbE",
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
